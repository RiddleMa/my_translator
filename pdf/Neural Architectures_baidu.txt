命名实体识别的神经网络体系结构
摘要
最先进的命名实体识别系统很大程度上依赖于手工制作的特征和特定领域的知识，以便从现有的小型、受监督的培训语料库中有效地学习。本文介绍了两种新的神经结构，一种是基于双向LSTM和条件随机场的神经结构，另一种是利用移位约简分析器启发的基于转换的方法构造和标记片段。
我们的模型依赖于两个关于单词的信息来源：从监督语料库中获取的基于字符的单词表示和从未注释语料库中获取的无监督的单词表示。我们的模型以四种语言获得NER的最新性能，而不需要借助于任何特定于语言的知识或资源，如地名录。
1引言
命名实体识别（NER）是一个富有挑战性的学习问题。一方面，在大多数语言和领域中，只有非常少量的监督培训数据可用。另一方面，对于可作为名称的单词的种类几乎没有限制，因此很难从这一小部分数据中进行归纳。因此，精心构造的正字法特征和特定于语言的知识资源，例如地名册，被广泛用于解决这一任务。不幸的是，在新的语言和新的领域中开发特定于语言的资源和特性是昂贵的，这使得NER成为适应的挑战。从未注释语料库进行无监督学习为从少量的监督中获得更好的概括性提供了一种替代策略。然而，即便是广泛依赖于非监督特性的系统（Collobert等人，2011；Turian等人，2010；Lin和Wu，2009；Ando和Zhang，2005b，等等）也使用这些特性来增强，而不是取代手工设计的特性（例如，关于资本化模式和特定语言中的字符类和专门的知识资源（例如，地名词典）。
在本文中，我们提出了NER的神经架构，除了少量的监控训练数据和未标记语料库之外，没有使用特定于语言的资源或特征。我们的模型被设计为捕捉两个直觉。首先，由于名称通常由多个令牌组成，因此联合对每个令牌的标记决策进行推理非常重要。这里我们比较了两个模型：（i）一个在其上面具有顺序条件随机层的双向LSTM（LSTM-CRF；2），以及（i i）一个新的模型，该模型使用基于转换的解析算法来构造和标记输入句子的块，该算法由栈LSTM（S-LSTM）表示；第3节）。第二，“作为名称”的令牌级证据包括两个拼写证据（标记为名称的单词看起来像什么？）分布证据（标注的词倾向于出现在语料库中）？为了捕获正字法灵敏度，我们使用基于字符的词表示模型(Ling等人，2015b)来捕获分布灵敏度，我们将这些表示与分布表示相结合(Mikolov等人，2013b)。我们的词表述结合了这两者，并且辍学培训被用来鼓励模型学习信任两个证据来源（4）。
英语、荷兰语、德语和西班牙语的实验表明，在荷兰语、德语和西班牙语中使用LSTM-CRF模型，我们能够获得最先进的NER性能，并且非常接近英语中的最新水平，而无需任何手工设计的特征或地名录（5）。基于转换的算法同样优于几种语言中最好的先前发表的结果，尽管其性能不如LSTM-CRF模型。
2 LSTM—CRF模型
我们提供了LSTM和CRFS的简要描述，并提出了一种混合标记结构。这种体系结构类似于Calbor等人提出的体系结构。（2011）和黄等人。（2015）。
2.1 LSTM
递归神经网络（RNNs）是一种基于顺序数据的神经网络。它们作为输入向量序列（x1，x2，.）。…并返回另一个序列（H1，H2，…）。…HN表示输入中每一步的序列信息。
虽然从理论上讲，RNN可以学习长期的依赖性，但在实践中，它们不能这样做，并且倾向于偏向于序列中最近的输入(Bengio等人，1994)。长期短期记忆网络（LSTM）被设计成通过结合记忆单元来克服这个问题，并且已经被证明能够捕获长期的依赖性。它们使用几个门来控制输入给存储单元的比例，以及从前一状态到忘记的比例（Hochreiter和Schmidhuber，1997）。我们使用以下实现：
it=_(Wxixt+Whiht_1+Wcict_1+bi)ct=(1_it)ct_1+it tanh(Wxcxt+Whcht_1+bc)ot=_(Wxoxt+Whoht_1+Wcoct+bo)ht=ot tanh(ct),
其中，西格玛是元素的S形函数，是元素的乘积。
对于一个给定的句子（x1，x2，.…xn)包含n个单词，每个单词表示为d维向量，LSTM计算每个单词t处句子左边上下文的表示ht。当然，生成右边上下文ht的表示还应该添加有用的信息。这可以通过使用反向读取相同序列的第二LSTM来实现。我们将前者称为前向LSTM，后者称为后向LSTM。这些是具有不同参数的两个截然不同的网络。这个前向和后向LSTM对被称为双向LSTM（Graves和Schmidhuber，2005）。
使用这个模型的单词的表示是通过连接其左右上下文表示ht=[ht;ht]。这些表示有效地包括上下文中单词的表示，这对于许多标记应用程序很有用。
2.2个CRF标记模型
一个非常简单但出人意料的有效标记模型是使用ht作为特性来为每个输出yt做出独立的标记决策（Ling等人，2015b）。尽管该模型在POS标记等简单问题中取得了成功，但是当输出标签之间有很强的依赖关系时，其独立的分类决策是有限的。NER就是这样的一个任务，因为表征标签的可解释序列的“语法”强加了几个硬约束（例如，I-PER不能遵循B-LOC；参见_2.4了解细节），这些约束不可能用独立假设建模。
因此，我们不是独立地对标记决策建模，而是使用条件随机字段对它们进行联合建模（Lafferty等人，2001）。输入语句
x=（x1，x2，.…，XN）
我们认为P是由双向LSTM网络输出的分数矩阵。P大小为n×k，其中k是不同标记的数目，Pi，j对应于句子中第i个单词的第j个标记的分数。对于一系列预测
y=（y1，y2，.）…，YN）
我们把它的分数定义为
S（x，y）＝nx i＝0，一，一，1＋nx i＝1π，i
其中A是转换分数的矩阵，使得A i，j表示从标记i到标记j.y0的转换分数，而yn是句子的开始和结束标记，我们将其添加到可能的标记集合中。因此A是大小为k＋2的正方形矩阵。
在所有可能的标签序列上的软最大值产生序列Y的概率：
p（y* x）＝PyeYe（XYE，YS）（X，YE）。
在训练过程中，我们最大化了正确的标签序列的对数概率：
log(p(y|X))=s(X,y)log_yeX∈YXes(X,ye)=s(X,y)logadd ye∈YXs(X,ye),(1)
其中YX表示句子X的所有可能的标记序列（甚至那些没有验证IOB格式的标记序列）。从上面的公式可以看出，我们鼓励我们的网络产生有效的输出标签序列。在解码时，我们预测输出序列，得到由以下给出的最大得分：
y＝AgMax yyyx s（x，yE）。（2）
由于我们只对输出之间的双ram相互作用进行建模，所以可以用动态规划来计算方程1中的和以及方程2中的最大后验序列y_
2.3参数化与训练
与每个令牌(即，Pi，y's)的每个标记决策相关联的分数被定义为在嵌入用双向LSTM计算的字上下文之间的点积——与Ling等人的POS标记模型完全相同。（2015b），这些与二元相容性分数（即，y，y0）相结合。这个体系结构如图1所示。圆表示观测变量，菱形表示其父函数的确定性函数，双圆表示随机变量。
图1：网络的主要体系结构。字嵌入被赋予双向LSTM。Li表示单词I及其左上下文，RI表示单词I及其右上下文。
级联这两个向量在其上下文CI中产生单词I的表示。
因此，该模型的参数是二进制兼容性得分A的矩阵，以及产生矩阵P的参数，即双向LSTM的参数、线性特征权重和字嵌入。在第2.2部分中，让XI表示句子中每个词的单词嵌入顺序，YI是它们的相关标记。我们返回讨论如何在第4节中对嵌入XI进行建模。单词嵌入序列作为双向LSTM的输入给出，双向LSTM返回每个单词的左右上下文的表示，如2.1所述。
这些表示被级联(ci)并线性地投影到大小等于不同标签数量的层上。我们不使用从该层输出的softmax，而是使用前面描述的CRF来考虑相邻的标签，从而产生每个单词yi的最终预测。此外，我们观察到在ci和CRF层之间添加隐藏层稍微改进了我们的结果。用这个模型报告的所有结果都包含了这个额外的层。在给定观察词的情况下，这些参数被训练成使带注释语料库中NER标签的观察序列的等式1最大化。
2.4种标记方案
命名实体识别的任务是将命名实体标签分配给句子中的每个单词。单个命名实体可以跨越一个句子中的几个令牌。句子通常以IOB格式（内部、外部、开始）表示，其中如果令牌是命名实体的开始，则每个令牌都被标记为B-label；如果令牌在命名实体内部，则将其标记为I-label，但不是命名实体内的第一个令牌，或者以其他方式标记为O。然而，我们决定使用IOBES标记方案，一种通常用于命名实体识别的IOB变体，它编码关于单个实体（S）的信息，并显式地标记命名实体（E）的结束。使用此方案，以高置信度将单词标记为I-label，缩小了后续单词到I-label或E-label的选择，然而，IOB方案只能确定后续单词不能是另一个标签的内部。拉蒂诺夫和罗斯（2009）和戴等。
（2015）表明，使用像IOBES这样更具表现力的标记方案可以稍微提高模型性能。然而，我们没有观察到IOB标签方案的显著改进。
3基于转换的组块模型
作为前一节中讨论的LSTM-CRF的替代，我们探索了一种新的体系结构，该体系结构使用类似于基于转换的依赖项解析的算法对输入序列进行分块和标记。该模型直接构造多令牌名称的表示（例如，名称Mark Watney被组合为单个表示）。
该模型依赖于堆栈数据结构来逐步构造输入的块。为了获得用于预测后续操作的堆栈的表示，我们使用Dyer等人提出的Stack-LSTM。（2015），其中LSTM用“堆栈指针”进行扩充。虽然顺序LSTMs从左到右对序列进行建模，但是堆栈LSTM允许嵌入对象堆栈，这些对象既被添加到（使用推送操作），又被从（使用弹出操作）中删除。这允许Stack-LSTM像栈一样工作，该栈维护“摘要嵌入”其内容。为了简单起见，我们将此模型称为堆栈LSTM或S－LSTM模型。
最后，我们向感兴趣的读者参考原始论文（Dyer等人，2015）以获得关于StackLSTM模型的详细信息，因为在本文中，我们只是通过下一节中提出的基于转换的新算法使用相同的体系结构。
3.1组块算法
我们设计了一个转换清单，如图2所示，它是受基于转换的解析器，特别是Nivre（2004）的弧形标准解析器的启发而设计的。
在这个算法中，我们使用两个堆栈（指定的输出和堆栈分别表示完成的块和划线空间）和一个包含有待处理的单词的缓冲区。
转换清单包含以下转换：SHIFT转换将单词从缓冲区移动到堆栈，OUT转换将单词从缓冲区直接移动到输出堆栈，而RE.E(y)转换将从堆栈顶部弹出所有项，从而创建“块”，用标签Y标记这个，并将这个块的表示推到输出堆栈上。当堆栈和缓冲区都是空的时候，算法就完成了。算法如图2所示，它显示了处理Mark Watney访问火星的句子所需的操作序列。
通过定义每个时间步骤中动作的概率分布，给定堆栈、缓冲区和输出的当前内容以及采取的动作的历史，对模型进行参数化。继戴尔等。（2015），我们使用堆栈LSTM来计算每个算法的固定维嵌入，并将它们进行级联以获得完整的算法状态。这个表示用于定义对每个时间步骤中可能采取的操作的分布。训练该模型以使给定输入句子的参考动作序列（从标记训练语料库中提取）的条件概率最大化。为了在测试时标记新的输入序列，贪婪地选择最大概率动作，直到算法达到终止状态。虽然这并不能保证找到全局最优，但它在实践中是有效的。由于每个令牌要么直接移动到输出（1个动作），要么首先移动到堆栈，然后是输出（2个动作），所以长度为n的序列的动作总数最大为2n。
图3：Mark Watney用堆栈LSTM模型访问Mars的过渡序列。
值得注意的是，这种算法模型的性质使得它对所使用的标记方案不可知，因为它直接预测标记块。
3.2表示标记块
当执行RE.E(y)操作时，算法将一系列令牌(连同它们的向量嵌入)从堆栈移到输出缓冲区作为单个完成的块。为了计算该序列的嵌入，我们在其组成令牌的嵌入上运行双向LSTM，以及表示被标识的块的类型（即，y）的令牌。这个函数给出为g（u，.）。…，V，RY），其中Ry是标签类型的习得嵌入。因此，输出缓冲区包含针对生成的每个标记块的单个向量表示，而不管其长度如何。
4输入Word Embeddings
我们两个模型的输入层是单个单词的向量表示。从有限的NER训练数据中学习单词类型的独立表示是一个难题：有太多的参数无法可靠地估计。
由于许多语言有正字法或形态学证据表明某物是名称（或不是名称），所以我们想要对单词的拼写敏感的表示。因此，我们使用一个模型，该模型从由(4.1)组成的字符的表示中构造单词的表示。我们的第二个直觉是，在大型语料库中，名称在规则上下文中出现，每个名称可能都各不相同。因此，我们使用从一个敏感语序的大语料库中学习的嵌入（4.2）。最后，为了防止模型过于依赖于一个表示或另一个表示，我们使用辍学训练，并发现这对于良好的泛化性能是至关重要的（4.3）。
图4：将“Mars”一词的字符嵌入到一个双向LSTM中。我们将其最后的输出连接到查找表中的嵌入，以获得这个词的表示。
基于4.1字的词汇模型
我们的工作与以往大多数方法的一个重要区别是，我们在训练时学习字符级别的特征，而不是手动工程前缀和后缀关于单词的信息。学习字符级嵌入具有学习特定于当前任务和领域的表示的优点。已经发现它们对于形态丰富的语言非常有用，并且能够处理词性标记和语言建模（Ling等人，2015b）或依赖解析（Balle.s等人，2015）等任务的词汇外问题。
图4描述了我们从字符生成单词嵌入的体系结构。随机初始化的字符查找表包含每个字符的嵌入。按照正向和向后LSTM的直接和反向顺序给出对应于单词中每个字符的字符嵌入。从字符派生的单词的嵌入是从双向LSTM中将单词的前向和后向表示连接起来。然后将此字符级表示与来自单词查找表的单词级表示进行连接。在测试期间，在查找表中没有嵌入的单词被映射到UNK嵌入。为了训练BIN嵌入，我们用概率0.5来替换单体嵌入。在我们的所有实验中，前向和后向字符LSTM的隐藏维数分别为25，这导致我们基于字符的词表示具有维数50。
像RNN和LSTM这样的递归模型能够编码很长的序列，然而，它们的表示偏向于它们最近的输入。因此，我们期望前向LSTM的最终表示是单词后缀的精确表示，而后向LSTM的最终状态是其前缀的更好表示。其他方法——最显著的是像卷积网络——已经被提议从它们的字符中学习单词的表示（Zhang等人，2015；Kim等人，2015）。然而，CuNETs被设计为发现它们的输入的位置不变特征。虽然这适用于许多问题，例如，图像识别（猫可以出现在图片中的任何地方），但我们认为重要的信息是位置相关的（例如，前缀和后缀编码与词干不同的信息），这使得LSTM成为用于建模的先验更好的函数类。词语与文字的关系。
4.2预训练嵌入
如科尔伯特等。（2011）我们使用预先训练的单词嵌入来初始化查找表。我们观察到显着的改进，使用预先训练的字嵌入随机初始化。
嵌入是使用skip-n-gram（Ling等人，2015a）进行预处理的，它是word2vec（Mikolov等人，2013a）的一个变体，用于解释词序。这些嵌入在训练过程中是微调的。
西班牙语、荷兰语、德语和英语的单词嵌入使用西班牙语Gigaword版本3、莱比锡语料库、来自2010年机器翻译研讨会的德语单语培训数据和英语Gigaword版本4（洛杉矶时报和纽约时报部分删除）进行培训。d)分别.2我们对英语使用100的嵌入维度，对其他语言使用64的嵌入维度，最小单词频率截止值为4，窗口大小为8。
4.3辍学训练
初步实验表明，字符级嵌入在与预训练字表示结合使用时并没有提高我们的总体性能。为了鼓励模型依赖于两种表示，我们使用辍学训练（Hinton等人，2012），在图1中双向LSTM的输入之前向最终嵌入层应用辍学掩码。我们观察到在使用辍学后，我们的模型的性能有了显著的改善（见表5）。
5个实验
本节介绍我们用于训练模型的方法、我们在各种任务上获得的结果以及网络配置对模型性能的影响。
5.1训练
对于提出的两个模型，我们使用反向传播算法来训练网络，在每个训练示例上更新参数，每次一个参数，使用学习率为0.01和梯度削波率为5.0的随机梯度下降（SGD）。已经提出了几种方法来提高SGD的性能，如Adadelta(Zeiler，2012)或Adam(Kingma and Ba，2014)。尽管我们使用这些方法观察到更快的收敛，但是它们都没有梯度剪切的SGD那么好。
我们的LSTM-CRF模型对于前向和后向LSTM使用单层，其尺寸设置为100。调整这个维度并没有显著影响模型性能。我们把辍学率设定为0.5。使用较高的利率负面影响我们的结果，而较小的利率导致较长的训练时间。
堆栈LSTM模型使用两个层，每个堆栈每个维度100。在组合函数中使用的动作的嵌入每个具有16个维度，并且输出嵌入具有20个维度。我们用不同的辍学率进行实验，并使用每种语言的最佳辍学率来报告分数。3这是一个贪婪的模型，它应用局部最优的动作直到整个句子被处理，进一步的改进可以通过波束搜索(Zhang和Clark，2011)或tra得到。随着探索（BalthestOS等，2016）。
5.2数据集
我们测试我们的模型在不同的数据集命名实体识别。为了证明我们的模型能够推广到不同的语言，我们在CoNLL-2002和CoNLL2003数据集（Tjong Kim Sang，2002；Tjong Kim Sang和De Meulder，2003）上给出了结果，这些数据集包含英语、西班牙语、德语和荷兰语的独立命名实体标签。所有数据集都包含四种不同类型的命名实体：位置、人员、组织和杂项实体，它们不属于前面三个类别中的任何一个。虽然POS标签对于所有数据集都是可用的，但是我们在模型中没有包含它们。
除了在英语NER数据集中用零替换每个数字之外，我们没有执行任何数据集预处理。
5.3结果
表1给出了与其他命名实体识别模型的比较。为了比较我们的模型和其他模型的公平性，我们报告了使用或不使用外部标记数据（如地名录和知识库）的其他模型的分数。我们的模型不使用地名或任何外部标记的资源。
这项任务的最佳成绩是罗素等。
（2015）。通过联合建模NER和实体链接任务，他们获得了91.2的F1（Hoffart等人，2011）。他们的模型使用了许多手工设计的特性，包括拼写特性、WordNet集群、Brown集群、POS标签、块标签，以及词干和外部知识库，如Freebase和Wikipedia。我们的LSTM-CRF模型优于所有其他系统，包括使用外部标记数据的系统，如地名录。除了Chiu和尼科尔斯（Nichols，2015）提出的模型之外，我们的StackLSTM模型还优于所有未包含外部特性的先前模型。
表2、表3和表4分别给出了我们关于德语、荷兰语和西班牙语的NER与其他模型的结果。在这三种语言中，LSTM-CRF模型的性能显著优于所有以前的方法，包括使用外部标记数据的方法。唯一的例外是荷兰人，吉利克等人的模型。（2015）可以通过利用来自其他NER数据集的信息来更好地执行。
与不使用外部数据的系统相比，Stack-LSTM还始终呈现状态艺术（或接近）结果。
如表所示，Stack-LSTM模型更依赖于基于字符的表示来实现竞争性能；我们假设LSTM-CRF模型需要较少的正交信息，因为它从双向LSTM中获得了更多的上下文信息；Stack-LSTM模型一个接一个地消耗单词，并且它在分块单词时仅仅依赖于单词表示。
5.4网络体系结构
我们的模型有几个组件，我们可以调整这些组件以了解它们对整体性能的影响。我们探讨了CRF、字符级表示、单词嵌入的预处理和辍学对LSTMCRF模型的影响。我们观察到，预处理字嵌入在F1中+7.31的整体性能方面给了我们最大的改进。CRF层增加了+1.79，而使用辍学导致+1.17的差异，最后学习字符级词嵌入导致+0.74的增加。对于堆栈LSTM，我们进行了一组类似的实验。表5给出了不同结构的结果。
表5：英文NER结果与我们的模型，使用不同的配置。“pre.”指包括预训练字嵌入的模型，“char”指包括基于字符的单词建模的模型，“dropout”指包括辍学率的模型。
6项相关工作
在CONLL2002共享任务中，卡雷拉斯等人。
(2002)通过组合几个小的固定深度决策树在荷兰和西班牙获得最佳结果。明年，在COLL2003共享任务中，弗洛里安等人。（2003）结合四种不同分类器的输出，获得德语最佳分数。齐等。(2009)随后，通过在大量未标记语料库上进行无监督学习，用神经网络对此进行了改进。
以前已经提出了一些其他神经架构的NER。例如，Collobert等人。（2011）在一个字嵌入的序列上使用美国有线电视新闻网，上面有CRF层。这可以看作是我们的第一个模型，没有字符级嵌入，并且双向LSTM被CNN替换。最近，黄等人。
（2015）提出了类似于我们的LSTM CRF的模型，但是使用手工制作的拼写特征。周和许（2015）也使用了相似的模型，并将其应用到语义角色标记任务中。Lin和Wu（2009）使用线性链CRF和L2正则化，他们添加了从web数据和拼写特征中提取的短语聚类特征。帕索斯等人。（2014）还使用具有拼写特征和地名词典的线性链CRF。
过去我们已经提出了与我们的语言无关的NER模型。Cucerzan和Yarowsky（1999；2002）提出了半监督引导算法，用于通过联合训练字符级（单词内部）和令牌级（上下文）特征来识别命名实体。艾森斯坦等。（2011）使用贝叶斯非参数来构建几乎不受监管的设置中的命名实体的数据库。Ratinov和Roth（2009）定量地比较几种NER方法，并使用规则化的平均感知器和聚合上下文信息构建它们自己的监督模型。
最后，目前对于使用基于字母的表示的NER模型有很多兴趣。
吉利克等人。(2015)将序列标记的任务建模为序列学习问题，并将基于字符的表示合并到它们的编码器模型中。Chiu和尼科尔斯（2015）采用了与我们类似的架构，但取而代之的是使用CNN学习角色级别的特性，其方式与Santos和Guimaraes（2015）的工作类似。
7结论
本文介绍了两种用于序列标记的神经结构，它们提供了在标准评估设置中报告的最好的NER结果，甚至与使用外部资源（如地名录）的模型相比。
我们的模型的一个关键方面是它们通过简单的CRF架构或者使用基于转换的算法显式地构造和标记输入的块，来对输出标签依赖关系建模。单词表示对于成功也很重要：我们使用预先训练的单词表示和“基于字符”的表示，它们捕获形态和正字法信息。为了防止学习者过分依赖于一个表示类，使用辍学。
致谢
这项工作部分由美国国防高级研究计划署（DARPA）信息创新办公室（I2O）根据DARPA/I2O根据HR0011-15-C-0114号合同发布的紧急事件低资源语言（LORELEI）方案赞助。Miguel Balle.s由欧洲委员会根据合同号FP7-ICT-610411（项目MULTISENSOR）和H2020-RIA-645012（项目KRISTINA）提供支持。
