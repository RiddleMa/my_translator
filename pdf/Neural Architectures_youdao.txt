命名实体识别的神经结构
摘要
最先进的命名实体识别系统严重依赖手工特性和特定领域的知识为了学习有效地从小型、监督训练语料是可用的。在本文中,我们引入了两个新的神经architectures-one基于双向LSTMs和条件随机域,构造和标签的,另一段使用跃迁过程方法受shift-reduce解析器。
我们的模型依赖于两个词:信息来源基于字符的字表示从监督主体和无监督从未经语料词表示。模型获取最先进的性能在尼珥四种语言而不诉诸任何特定于语言的知识或资源(如地名表。
1介绍
命名实体识别(尼珥)是一个具有挑战性的学习问题。一方面,大多数语言和领域,只有少量的监督训练数据可用。另一方面,很少有限制的词可以名字,所以推广从这个小样本数据是很困难的。因此,精心构建拼写特点和特定于语言的知识资源,如地名表,广泛用于解决这一任务。不幸的是,languagespecific资源和功能是昂贵的发展在新的语言和新的领域,使尼珥适应的一个挑战。无监督学习从未经语料库提供的另外一个策略来获得更好的泛化从少量的监督。然而,即使系统广泛依赖无监督特征(Collobert et al ., 2011;Turian et al .,2010;林和吴,2009;安藤和张,2005 b,尤其)使用这些增强,而不是取代,hand-engineered特性(例如,知识资本化模式和在一个特定的字符类语言)和专业知识资源(如地名表)。
在本文中,我们目前的神经结构尼珥不使用特定于语言的资源或功能除了少量的监督训练数据和标记全集。我们的模型是用来捕获两个直觉。首先,由于名称通常由多个令牌,对标签决策推理共同为每个令牌是很重要的。我们比较两个模型,(我)一个双向LSTM连续条件随机层上面(LSTM-CRF;§2),和(2)一个新的模型,构造和标签的输入块的句子使用一种算法受跃迁过程解析与州代表堆栈LSTMs(S-LSTM;§3)。第二,标记级证据“名称”既包括拼写证据(这个词被标记为一个名字什么样子?)和分配的证据(这个词在哪里被标记往往发生在语料库?)。捕捉正字法的敏感性,我们使用基于字符的字表示模型(凌et al ., 2015 b)来捕获灵敏度分布,我们结合这些交涉分布表征(Mikolov et al ., 2013 b)。结合这两个词表示,辍学训练是用来鼓励模型学会信任证据的来源(§4)。
实验用英语,荷兰语,德语,西班牙语表明我们能够获得先进尼珥饮片性能与LSTM-CRF模型在荷兰,德国,西班牙,非常先进的英语没有任何hand-engineered特性或附近地名表(§5)。跃迁过程算法同样超过之前最好的结果发表在几种语言,尽管它比LSTM-CRF模型上就不可能有好的表现。
2 LSTM-CRF模型
我们提供一个简短的描述LSTMs控,和现在的混合标记架构。这种架构类似于那些由Collobert et al。(2011)和黄et al。(2015)。
2.1 LSTM
复发性神经网络(RNNs)是一个家庭的操作顺序数据的神经网络。他们作为输入向量序列(x1,x2)。xn)并返回另一个序列(h1,h2,。,hn)代表一些信息序列中每一步的输入。
尽管RNNs可以,理论上,学习长依赖关系,在实践中他们不这样做,往往会偏向他们最近输入的序列(Bengio et al .,1994)。长期短期记忆网络(LSTMs)旨在解决这一问题,通过融合一个存储单元和已被证明捕获远程依赖关系。他们这样做,使用几个门,控制输入的比例给记忆细胞,和先前的比例状态忘记(1997年,的Hochreiter和。施密德胡贝尔表示)。我们使用下面的实现:
它=σ(Wxixt + Whiht−1 + Wcict−1 + bi)ct =(1−)ct−1 +双曲正切(Wxcxt + Whcht−1 + bc)ot =σ(Wxoxt + Whoht−1 + Wcoct + bo)ht = ot双曲正切(ct),
其中σ是element-wise乙状结肠功能,是element-wise产品。
对于一个给定的句子(x1,x2)。xn)包含n个单词,每个表示为采用向量,一个LSTM计算表示ht离开上下文的句子的每个字t。当然,生成正确的表示上下文ht也应该添加有用的信息。这可以通过使用第二个LSTM读取相同的顺序相反。我们将把前者称为向前向后LSTM LSTM,后者。这些是不同的两个不同的网络参数。这前后LSTM一对称为双向LSTM(2005年,的坟墓和。施密德胡贝尔表示)。
使用这个模型的表示一个词是通过连接左右上下文表示,ht =[−→ht;←−ht)。这些表示有效的表示一个词包含在上下文,它是有用的为许多标签的应用程序。
2.2 CRF标记模型
很简单,但令人惊讶的是effective-tagging模型是使用ht的特性做出独立的标签决定为每个输出欧美(凌et al .,2015 b)。尽管这种模式的成功在词类等简单问题,其独立分类决策是限制在输出标签当有很强的依赖性。尼珥就是这样一个任务,因为“语法”是可翻译的序列标签强加一些硬约束(例如,自己演出不能遵循B-LOC;有关详细信息,请参阅§2.4)是不可能与独立的假设模型。
因此,而不是建模标记独立决策,我们共同使用一个条件随机场模型(拉弗蒂et al .,2001)。对于一个输入句子
X =(x1,x2)。xn),
我们认为P矩阵得分输出的双向LSTM网络。P是大小n×k,k是不同的标签的数量,和π,j对应分数j标记的第i个单词一个句子中去。序列的预测
y = (y1, y2, . . . , yn),
我们定义它的分数
s(X,y)= nX我= 0易建联,易建联+ 1 + nX i = 1π,易建联
是一个矩阵的转换分数Ai,j代表从标签的分数我标记j . y0和yn是一个句子的开始和结束标记,我们添加一组可能的标签。因此一个方阵的大小k + 2。
在所有可能的标签序列softmax y收益率序列的概率:
p(y | X)=派伊∈的眼睛(XXe y)(X,你们)。
在培训期间,我们正确的标签序列的对数概率最大化:
日志(p(y | X))= s(X,y)−日志yeX∈YX es(X,你们)= s(X,y)−logadd你们∈YX s(X,你们)(1)
,y代表所有可能的标签序列(甚至那些不验证入会格式)的一个句子x上面的配方中,很明显,我们鼓励我们的网络产生一个有效的输出序列标签。解码时,我们预测的输出序列,获得最高分数:
y∗= argmax你们∈YX s(X,你们)。(2)
因为我们只有三元输出之间的交互建模,情商的总和。1和最大后验序列y∗Eq。2可以使用动态规划计算。
2.3参数化和培训
每个令牌的分数与每个标签关联的决定(即。π,y)定义的点积的嵌入wordin-context计算双向LSTM——一模一样的词类模型凌et al .(2015 b),这些都是结合三元兼容性分数(即。,唉,y0)。这个体系结构如图1所示。圆圈代表观察到的变量,钻石是父母确定性函数和双圈是随机变量。
图1:网络的主要架构。字嵌入的双向LSTM给出。李代表这个词我和它的背景下,国际扶轮代表这个词我和它的上下文。
连接这两个向量收益率的表示我在其上下文中,ci。
这个模型的参数矩阵元兼容性分数,并产生矩阵P的参数,即双向LSTM的参数、线性特性重量、“嵌入”这个词。2.2部分,让习近平表示词的序列嵌入一个句子的每一个字,和彝语相关的标签。我们回到讨论如何建模嵌入的习近平在第四节。给出的单词序列嵌入的双向LSTM作为输入,返回一个表示左派和右派的上下文2.1解释每个单词。
这些表示连接(ci)和线性投射到一层,其大小等于不同标签的数量。而不是使用将softmax输出这一层,我们使用一个CRF如前所述考虑到邻近的标签,产生最终的预测易建联的每一个字。另外,我们观察到,ci和CRF层之间增加一个隐藏层略微改善我们的结果。所有与该模型结果报告将这层。最大化Eq。1的参数训练观察尼珥序列标签标注语料库,观察到的单词。
2.4标签计划
命名实体识别的任务是将一个命名实体标签分配给每一个单词一个句子中去。一个命名实体可以跨几个标记在一个句子。句子通常表现在入会格式(内,外,开始),每一个令牌贴上B-label如果令牌是一个命名实体的开始,我标榜里面如果是命名实体,但不是第一个令牌在命名实体,或O。然而,我们决定使用入会标签计划,约伯的变种常用的命名实体识别,编码单信息实体(S)和明确的结束标志着命名实体(E)。使用这种方案,标记一个单词是我标榜高信任度缩小了随后的词我标榜的选择或E-label,然而,约伯的计划仅仅是能够确定后续的词不能内部的另一个标签。Ratinov和罗斯(2009)和戴秉国等。
(2015)表明,使用更具表达性的标记方案比如入会改善模型性能。然而,我们没有观察到显著提高入会标签计划。
3跃迁过程分块模型
作为一个替代LSTM-CRF在前一节中所讨论的,我们探索一个新的架构,块和标签的输入序列使用一种算法类似于跃迁过程依赖解析。这个模型的直接构造表示multi-token名称(例如,马克·沃特尼是组合成一个单一的名称表示)。
这个模型依赖于一个堆栈数据结构逐步构建块的输入。获得这个堆栈用于预测后续行为的表征,我们使用的Stack-LSTM代尔et al .(2015),LSTM是增强“堆栈指针。“虽然从左到右顺序LSTMs模型序列,堆栈LSTMs允许嵌入一堆对象添加到(使用推操作)和删除(使用流行的操作)。这允许Stack-LSTM工作像一个堆栈,维护一个“总结嵌入”的内容。我们称这种模式为Stack-LSTM或S-LSTM模型简单。
最后,我们有兴趣的读者参考原来的纸(代尔et al .,2015)以来细节StackLSTM模型在本文中,我们只是使用相同的体系结构通过一个新的跃迁过程算法在以下部分。
3.1分块算法
我们设计了一个过渡提供库存受跃迁过程解析器在图2中,尤其是arc-standard Nivre的解析器(2004)。
在这个算法中,我们使用两个堆栈(指定输出和堆栈代表,分别完成块和划痕空间)和缓冲区包含词尚未处理。
过渡库存包含以下转变:转变过渡移动堆栈缓冲区的一句话,直接从缓冲过渡动作一个词到输出栈而减少(y)过渡弹出所有项目从堆栈的顶部创建一个“块”,标签与标签y,并把这一块的表示到输出栈。该算法完成当堆栈和缓冲区都是空的。算法是图2所示,它显示了所需的操作序列处理句子马克·沃特尼访问火星。
模型参数化通过定义一个概率分布在行动在每个时间步,鉴于当前堆栈的内容,缓冲区,和输出,以及行动的历史。代尔et al。(2015),我们使用堆栈LSTMs来计算一个固定空间嵌入这些,并连接这些国家获得完整的算法。这种表示方法是用来定义一个分布在可能在每个时间步可以采取的行动。模型训练的参考序列的条件概率最大化行为(从一个标记训练语料库中提取)给定输入的句子。在测试时间标签一个新的输入序列,最大概率选择行动贪婪地到达终止状态,直到算法。虽然这是不能保证找到全局最优,它在实践中是有效的。因为每个令牌是直接移动到输出(1)或第一个堆栈,然后输出(2操作),操作序列的长度的总数n是最大2 n。
图3:过渡序列马克·沃特尼访问火星与Stack-LSTM模型。
值得注意的是,该算法模型的性质使它不可知论者使用的标记方案,因为它直接预测标记块。
3.2代表标记块
减少(y)操作执行时,该算法变化序列的令牌(连同他们的向量映射进行)从堆栈到输出缓冲区作为单个块完成。计算一个嵌入的序列,我们运行一个双向LSTM在一起组成的嵌入的令牌一块令牌代表的类型被确定(即。,y)。这个函数是作为g (u。,v,ry),从一个有学问的一个标签类型的嵌入。因此,输出缓冲区包含一个为每个标记块生成的向量表示,不管它的长度。
4输入字嵌入
输入层的模型是向量表示的单词。学习独立表征词类型从有限的尼珥训练数据是一个困难的问题:有太多的参数能可靠地估计。
因为很多语言都有拼写或形态学证据表明,有一个名称(或者不是一个名字),我们希望表示敏感词语的拼写。因此,我们使用一个模型结构表示的单词表示的字符组成(4.1)。我们第二个直觉是,名字,单独非常不同,在常规情况下出现在大全集。因此我们使用嵌入的从一个大型语料库,敏感词(4.2)。最后,为了防止模型根据一种表示太强烈,我们使用辍学培训和发现这对泛化性能好(4.3)是至关重要的。
图4:这个词的字符映射进行双向LSTMs给出“火星”。我们连接他们的最后输出的嵌入一个查找表来获得这个词表示。
4.1基于字符模式的单词
从大多数先前的方法我们工作的一个重要的区别是,我们学习字符级功能训练,而不是hand-engineering前缀和后缀词的信息。学习字符级嵌入的优点学习表示特定于手头的任务和域。他们已经发现有用的形态丰富的语言和处理outof-vocabulary问题等任务的词性标注和语言建模(凌et al .,2015 b)或依赖项解析(Ballesteros et al .,2015)。
图4描述了我们的架构来生成一个字嵌入一个字从它的字符。随机字符查找表初始化包含一个嵌入每个角色。每个角色对应的字符嵌入在一个词给出了直接和倒序LSTM向前和向后。这一词源于其人物的嵌入前后的连接从双向LSTM表示。然后该字符级表示连接的句表示词按照查询表。在测试过程中,单词没有嵌入在查找表映射到UNK嵌入。训练UNK嵌入,我们单件替换为UNK嵌入一个概率为0.5。在所有的实验中,隐藏的维度向前和向后的每个字符LSTMs 25,导致我们的基于字符的单词表示的尺寸50。
复发模型RNNs和LSTMs能够编码很长时间序列,然而,他们有一个表示偏向他们最近的输入。因此,我们期望的最后表示向前LSTM后缀的词,一个精确的表示和最终状态的向后LSTM更好的表现其前缀。替代方法——尤其是像卷积网络提出学习单词表示的字符(Zhang et al .,2015;金正日et al ., 2015)。然而,回旋网旨在发现position-invariant特性的输入。虽然这是适合很多问题,例如,图像识别(一只猫可以出现在任何图片),我们认为是位置相关的重要信息(例如,前缀和后缀不同的信息编码比茎),使得建模LSTMs一个先验更好的函数类单词和他们的角色之间的关系。
4.2 Pretrained嵌入
如Collobert et al。(2011),我们使用嵌入的初始化pretrained词查找表。我们观察到显著改善使用嵌入pretrained词在随机初始化的。
嵌入使用skip-n-gram pretrained(凌et al ., 2015)的一个变体word2vec (Mikolov et al ., 2013),占词序。在训练这些映射进行调整。
字嵌入,西班牙语,荷兰语,德语和英语培训使用西班牙Gigaword版本3,莱比锡语料收集,德国语训练数据从2010年车间机器翻译和英语Gigaword version 4(《洛杉矶时报》和《纽约时报》部分删除)。2我们使用一个嵌入维100年英语,64年为其他语言,最小词频截止4和8的窗口大小。
4.3辍学培训
初步实验表明,字符级嵌入使用时没有提高我们的整体性能与pretrained词表示。鼓励模型依赖都表示,我们使用辍学培训(辛顿et al .,2012),应用辍学面具最终嵌入层之前的输入双向LSTM如图1所示。我们观察到显著改善我们的模型的性能在使用辍学(见表5)。
5实验
本节介绍了我们使用的方法来训练我们的模型,我们得到的结果在不同的任务,我们的网络的配置对模型性能的影响。
5.1培训
两个模型,我们使用反向传播算法训练网络更新参数对每个训练的例子,一次一个,使用随机梯度下降法(SGD)学习速率为0.01和5.0的梯度剪裁。提出了几种方法来提高SGD的性能,如Adadelta(Zeiler,2012)或亚当(Kingma和Ba,2014)。虽然我们观察使用这些方法收敛快,没有人执行以及与梯度SGD剪裁。
LSTM-CRF模型采用的是单层的向前和向后LSTMs的尺寸设置为100。模型的性能调优这个维度没有显著影响。我们设置了辍学率至0.5。使用更高的利率的负面影响我们的结果,而较小的利率导致较长的训练时间。
stack-LSTM模型使用两层每个尺寸100每个堆栈。使用的行为构成函数的映射进行16尺寸,和输出嵌入维度20。我们尝试了不同的辍学率和报告的分数为每种语言使用最好的辍学率。三是一个贪婪的模型,应用局部最优行动,直到整个句子处理,可获得进一步改善与定向搜索(Zhang和克拉克,2011年)或训练探索(Ballesteros et al ., 2016)。
5.2数据集
我们测试我们的模型在不同的数据集的命名实体识别。为了证明我们的模型推广到不同的语言的能力,我们现在结果conll - 2002和CoNLL2003数据集(Tjong金唱,2002;Tjong金唱、德Meulder 2003)包含独立命名实体标签英语、西班牙语、德语和荷兰语。所有数据集包含四种不同类型的命名实体:地点、人员、组织、和其他实体,不属于任何三个类别。尽管POS标签可用数据集,我们没有包括在我们的模型。
我们没有执行任何数据集预处理,除了取代英语的每一个数字以零尼珥数据集。
5.3结果
表1给出了比较与其他模型用英语命名实体识别。让我们的模型之间的比较和其他公平,我们报告的其他模型使用和不使用外部标记数据地名表和知识库等。我们的模型不使用地名表或任何外部标记资源。
最好的分数报道这个任务是由罗等。
(2015)。他们获得了F1的91.2联合建模尼珥和实体链接任务(Hoffart et al ., 2011)。他们的模型使用了很多hand-engineered特性包括拼写特点,WordNet集群,棕色的集群,POS标签,块标签,以及阻止外部知识库,如毒品和维基百科。LSTM-CRF模型优于所有其他系统,包括使用外部的标签数据像地名表。StackLSTM模型也优于所有先前的模型不包含外部特性,除了赵所呈现的一个和尼科尔斯(2015)。
表2、3和4尼珥提出我们的结果对德国、荷兰和西班牙分别相比其它模型。在这三种语言,LSTM-CRF模型明显优于所有以前的方法,包括使用外部标记数据。唯一的例外是荷兰,Gillick这样et al .(2015)的模型可以进行更好的利用信息从其他尼珥数据集。
Stack-LSTM也一直呈现statethe-art(或接近)结果相比不使用外部数据的系统。
表中我们可以看到,Stack-LSTM模型更依赖于基于字符的表示来实现竞争性能;我们假设LSTM-CRF模型需要较少的正字法的信息因为它得到更多的上下文信息的双向LSTMs;然而,Stack-LSTM模型使用单词,只是依赖这个词表示块的话。
5.4网络体系结构
我们的模型有几个组件,我们可以调整理解他们对整体性能的影响。我们探讨了影响CRF,字符级表示,pretraining词LSTMCRF模型嵌入和辍学。我们观察到pretraining字嵌入的给了我们最大的改善总体性能在F1 + 7.31。CRF层给我们增加+ 1.79,在使用辍学导致1.17 +的区别,最后学习字符级字嵌入导致增加约为+ 0.74。Stack-LSTM我们执行一组类似的实验。结果在表5给出不同的体系结构。
表5:英语尼珥的结果与我们的模型,使用不同的配置。“pretrain”是指嵌入模型,包括pretrained词,“字符”是指模型,包括基于字符建模的话,“辍学”是指模型,包括辍学率。
6相关工作
在conll - 2002共享任务,卡雷拉斯et al。
(2002)中获得最好的结果在荷兰和西班牙的几个小固定高度结合决策树。明年,CoNLL2003共享任务,Florian et al。(2003)获得最好的分数在德国通过结合四个不同分类器的输出。Qi et al。(2009)后改善这个神经网络通过无监督学习在一个巨大的无标号语料库。
其他几个神经结构以前提出了尼珥。例如,Collobert et al。(2011)使用一个CNN的序列词与CRF嵌入层之上。这可以被认为是我们的第一个模型没有字符级嵌入和双向LSTM由CNN所取代。最近,黄等。
(2015)提出了一个模型我们LSTM-CRF相似,但使用手工拼写功能。周和徐(2015)也使用类似的模型和适应语义角色标注任务。林和吴(2009)使用一个线性链CRF L2正规化,他们补充说短语集群功能从网络中提取数据和拼写功能。Passos et al。(2014)也使用一个线性链CRF拼写功能和地名表。
像我们这样的语言独立的尼珥模型也被提出。Cucerzan和Yarowsky (1999;2002)目前semi-supervised引导为命名实体识别算法co-training字符级(词)和标记级(上下文)特性。艾森斯坦et al。(2011)使用贝叶斯非参数来构造数据库的命名实体几乎无人监督的设置。Ratinov和罗斯(2009)定量比较尼珥的几种方法和建立自己的监督模型使用一个正规化的平均感知器和聚合上下文信息。
最后,目前很多兴趣模型使用letter-based表示尼珥。
Gillick这样et al。(2015)模型的任务sequencelabeling序列,序列学习问题,将基于字符表示纳入编码器模型。赵和尼科尔斯(2015)使用一个架构类似于我们的,而是运用cnn学习字符级特性,在某种程度上类似于由桑托斯和吉马良斯(2015)。
结论˜7
介绍两个神经结构序列标签提供最好的尼珥结果报道在标准评价设置,甚至与模型相比,使用外部资源,如地名表。
我们的模型的一个关键方面是他们依赖模型输出标签,通过一个简单的CRF的架构,或使用一个跃迁过程算法来显式地构造和标签块的输入。词表示成功也至关重要:我们使用pre-trained词表示和基于字符”表示捕捉形态和正字法的信息。防止学习者过于依赖一种表示类,辍学。
致谢
这项工作的部分赞助由美国国防高级研究计划局(DARPA)信息创新办公室(I2O)在低资源语言紧急事件(罗蕾莱)计划发行的美国国防部高级研究计划局/ I2O号合同下。hr0011 - 15 - c - 0114。米格尔Ballesteros支持欧盟委员会根据合同数量fp7 - ict - 610411(项目多传感器)和之下h2020 - ria - 645012(KRISTINA项目)。
